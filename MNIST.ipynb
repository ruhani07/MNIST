{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1xRoKOcPTz-k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader # for dividing data into small chunks\n",
        "from torchvision import datasets # we will be getting data sets from theis module\n",
        "from torchvision.transforms import ToTensor # the data we will get will be in the form of image using this we will convert it to tensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = datasets.MNIST(\n",
        "    root = \"data\",\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = ToTensor()\n",
        ")\n",
        "\n",
        "test_set = datasets.MNIST(\n",
        "    root = \"data\",\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "-DoFqU9cUTeH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2f5620d-31f1-4f01-9086-81aa4710370d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.04MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 134kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:06<00:00, 243kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.08MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_set, batch_size = 128, shuffle = True)\n",
        "test_dataloader = DataLoader(test_set, batch_size = 128, shuffle = True)"
      ],
      "metadata": {
        "id": "8HPLXshGU_H4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn # neural network library i need\n",
        "\n",
        "class MNIST_MLP(nn.Module): # this class has to be a child of nn.Module\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten() # convert the matrix to a vector\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28,512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,512), # i can choose any number of intermediate node its totally random\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,10)\n",
        "    ) # nn.Sequential creates a pipeline\n",
        "\n",
        "  def forward(self,x): # forward propogation\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "PDvDPXBuVxKb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "#if cuda is available then use cuda otherwise cpu\n",
        "model = MNIST_MLP().to(device)\n",
        "print(model)\n",
        "#create instance of  MNIST_MLP , to(device) if you use network is ported into gpu and print the model structure"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiGDLq9rGiLz",
        "outputId": "adeb13bc-bb41-4cf5-fb14-dd7043a51d36"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "MNIST_MLP(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets create random data\n",
        "x = torch.rand(1,28,28,device = device)\n",
        "logits = model(x)\n",
        "print(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfUsPUE0HFFK",
        "outputId": "a5a0c7e6-df5d-4f94-a36f-aa0b7472a9b0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0360,  0.0120, -0.0069, -0.0415,  0.0558,  0.0210,  0.1261, -0.0029,\n",
            "         -0.0372,  0.0680]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create this data into probabilties\n",
        "pred_prob = nn.Softmax(dim = 1)(logits)\n",
        "print(pred_prob)\n",
        "# dim=1 means we use it as a column vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6Qqw1BqHxVN",
        "outputId": "47231472-faf8-4dad-ce48-88115dcbd9b6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0948, 0.0995, 0.0976, 0.0943, 0.1039, 0.1004, 0.1115, 0.0980, 0.0947,\n",
            "         0.1052]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training parameters\n",
        "\n",
        "learning_rate = 1e-3 # 0.001\n",
        "max_epochs = 30\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
      ],
      "metadata": {
        "id": "pLTmSGkHIAab"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_mlp(dataloader, model, loss_fn, optimizer,max_epochs):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "\n",
        "  for epoch in range(max_epochs):\n",
        "    for batch, (X,y) in enumerate(dataloader):\n",
        "      X,y = X.to(device), y.to(device) # porting from CPU to GPU\n",
        "      pred = model(X) # Forward Prapogation\n",
        "      loss = loss_fn(pred,y) # Loss Calculation\n",
        "\n",
        "      optimizer.zero_grad() # gradient reset to zero\n",
        "      loss.backward() # gradient computation\n",
        "      optimizer.step() # weight updation\n",
        "      if batch % 100 == 0:\n",
        "        loss ,current = loss.item(), batch * len(X)\n",
        "        print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_mlp(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  model.eval()\n",
        "  test_loss, correct = 0,0\n",
        "  for X,y in dataloader:\n",
        "    X,y = X.to(device), y.to(device)\n",
        "    pred = model(X)\n",
        "    test_loss += loss_fn(pred,y).item()\n",
        "    correct += (pred.argmax(1) == y).type(torch.float).sum().item()# index of the max probability same as actual label\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "train_mlp(train_dataloader, model, loss_fn, optimizer, max_epochs)\n",
        "test_mlp(test_dataloader, model, loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS0legTyIJF4",
        "outputId": "3b8de9ca-cf60-4a7b-a581-804b6889c3a5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.310499 [    0/60000]\n",
            "loss: 2.300726 [12800/60000]\n",
            "loss: 2.294435 [25600/60000]\n",
            "loss: 2.287180 [38400/60000]\n",
            "loss: 2.287267 [51200/60000]\n",
            "loss: 2.281113 [    0/60000]\n",
            "loss: 2.285122 [12800/60000]\n",
            "loss: 2.268475 [25600/60000]\n",
            "loss: 2.274605 [38400/60000]\n",
            "loss: 2.268339 [51200/60000]\n",
            "loss: 2.263160 [    0/60000]\n",
            "loss: 2.255666 [12800/60000]\n",
            "loss: 2.254948 [25600/60000]\n",
            "loss: 2.238776 [38400/60000]\n",
            "loss: 2.242610 [51200/60000]\n",
            "loss: 2.235823 [    0/60000]\n",
            "loss: 2.229156 [12800/60000]\n",
            "loss: 2.217221 [25600/60000]\n",
            "loss: 2.200912 [38400/60000]\n",
            "loss: 2.209281 [51200/60000]\n",
            "loss: 2.201340 [    0/60000]\n",
            "loss: 2.185786 [12800/60000]\n",
            "loss: 2.172340 [25600/60000]\n",
            "loss: 2.157135 [38400/60000]\n",
            "loss: 2.166742 [51200/60000]\n",
            "loss: 2.135669 [    0/60000]\n",
            "loss: 2.151598 [12800/60000]\n",
            "loss: 2.113892 [25600/60000]\n",
            "loss: 2.117150 [38400/60000]\n",
            "loss: 2.110643 [51200/60000]\n",
            "loss: 2.090275 [    0/60000]\n",
            "loss: 2.091150 [12800/60000]\n",
            "loss: 2.071739 [25600/60000]\n",
            "loss: 2.039923 [38400/60000]\n",
            "loss: 2.037581 [51200/60000]\n",
            "loss: 1.968176 [    0/60000]\n",
            "loss: 1.969831 [12800/60000]\n",
            "loss: 1.981891 [25600/60000]\n",
            "loss: 1.923994 [38400/60000]\n",
            "loss: 1.921087 [51200/60000]\n",
            "loss: 1.913971 [    0/60000]\n",
            "loss: 1.854422 [12800/60000]\n",
            "loss: 1.838273 [25600/60000]\n",
            "loss: 1.800721 [38400/60000]\n",
            "loss: 1.755986 [51200/60000]\n",
            "loss: 1.726290 [    0/60000]\n",
            "loss: 1.750613 [12800/60000]\n",
            "loss: 1.668819 [25600/60000]\n",
            "loss: 1.673478 [38400/60000]\n",
            "loss: 1.631838 [51200/60000]\n",
            "loss: 1.576500 [    0/60000]\n",
            "loss: 1.550978 [12800/60000]\n",
            "loss: 1.415528 [25600/60000]\n",
            "loss: 1.451414 [38400/60000]\n",
            "loss: 1.427684 [51200/60000]\n",
            "loss: 1.385847 [    0/60000]\n",
            "loss: 1.454227 [12800/60000]\n",
            "loss: 1.399536 [25600/60000]\n",
            "loss: 1.274151 [38400/60000]\n",
            "loss: 1.250120 [51200/60000]\n",
            "loss: 1.313579 [    0/60000]\n",
            "loss: 1.266404 [12800/60000]\n",
            "loss: 1.222920 [25600/60000]\n",
            "loss: 1.159325 [38400/60000]\n",
            "loss: 1.131348 [51200/60000]\n",
            "loss: 1.154799 [    0/60000]\n",
            "loss: 1.086601 [12800/60000]\n",
            "loss: 1.039953 [25600/60000]\n",
            "loss: 1.110399 [38400/60000]\n",
            "loss: 1.059296 [51200/60000]\n",
            "loss: 1.040059 [    0/60000]\n",
            "loss: 1.080008 [12800/60000]\n",
            "loss: 0.957700 [25600/60000]\n",
            "loss: 0.938345 [38400/60000]\n",
            "loss: 0.967085 [51200/60000]\n",
            "loss: 0.858890 [    0/60000]\n",
            "loss: 0.919355 [12800/60000]\n",
            "loss: 0.867865 [25600/60000]\n",
            "loss: 0.866333 [38400/60000]\n",
            "loss: 0.874506 [51200/60000]\n",
            "loss: 0.777523 [    0/60000]\n",
            "loss: 0.778613 [12800/60000]\n",
            "loss: 0.806849 [25600/60000]\n",
            "loss: 0.798102 [38400/60000]\n",
            "loss: 0.890132 [51200/60000]\n",
            "loss: 0.834906 [    0/60000]\n",
            "loss: 0.670768 [12800/60000]\n",
            "loss: 0.659261 [25600/60000]\n",
            "loss: 0.734980 [38400/60000]\n",
            "loss: 0.821064 [51200/60000]\n",
            "loss: 0.869702 [    0/60000]\n",
            "loss: 0.723853 [12800/60000]\n",
            "loss: 0.741603 [25600/60000]\n",
            "loss: 0.796717 [38400/60000]\n",
            "loss: 0.692751 [51200/60000]\n",
            "loss: 0.689838 [    0/60000]\n",
            "loss: 0.780686 [12800/60000]\n",
            "loss: 0.728951 [25600/60000]\n",
            "loss: 0.684238 [38400/60000]\n",
            "loss: 0.637665 [51200/60000]\n",
            "loss: 0.693150 [    0/60000]\n",
            "loss: 0.675045 [12800/60000]\n",
            "loss: 0.656236 [25600/60000]\n",
            "loss: 0.678803 [38400/60000]\n",
            "loss: 0.708694 [51200/60000]\n",
            "loss: 0.703219 [    0/60000]\n",
            "loss: 0.579967 [12800/60000]\n",
            "loss: 0.629619 [25600/60000]\n",
            "loss: 0.628333 [38400/60000]\n",
            "loss: 0.761786 [51200/60000]\n",
            "loss: 0.636228 [    0/60000]\n",
            "loss: 0.537035 [12800/60000]\n",
            "loss: 0.568030 [25600/60000]\n",
            "loss: 0.518073 [38400/60000]\n",
            "loss: 0.546353 [51200/60000]\n",
            "loss: 0.597138 [    0/60000]\n",
            "loss: 0.631956 [12800/60000]\n",
            "loss: 0.469674 [25600/60000]\n",
            "loss: 0.658108 [38400/60000]\n",
            "loss: 0.533901 [51200/60000]\n",
            "loss: 0.626277 [    0/60000]\n",
            "loss: 0.490852 [12800/60000]\n",
            "loss: 0.484977 [25600/60000]\n",
            "loss: 0.630550 [38400/60000]\n",
            "loss: 0.515999 [51200/60000]\n",
            "loss: 0.516131 [    0/60000]\n",
            "loss: 0.702240 [12800/60000]\n",
            "loss: 0.519462 [25600/60000]\n",
            "loss: 0.637032 [38400/60000]\n",
            "loss: 0.555912 [51200/60000]\n",
            "loss: 0.435457 [    0/60000]\n",
            "loss: 0.520362 [12800/60000]\n",
            "loss: 0.605964 [25600/60000]\n",
            "loss: 0.551904 [38400/60000]\n",
            "loss: 0.478891 [51200/60000]\n",
            "loss: 0.610816 [    0/60000]\n",
            "loss: 0.606811 [12800/60000]\n",
            "loss: 0.552286 [25600/60000]\n",
            "loss: 0.481898 [38400/60000]\n",
            "loss: 0.546008 [51200/60000]\n",
            "loss: 0.412950 [    0/60000]\n",
            "loss: 0.487553 [12800/60000]\n",
            "loss: 0.576809 [25600/60000]\n",
            "loss: 0.490001 [38400/60000]\n",
            "loss: 0.371131 [51200/60000]\n",
            "loss: 0.444706 [    0/60000]\n",
            "loss: 0.555456 [12800/60000]\n",
            "loss: 0.398136 [25600/60000]\n",
            "loss: 0.393780 [38400/60000]\n",
            "loss: 0.352189 [51200/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.0%, Avg loss: 0.460885 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet_MNIST(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1,6,5) # in-channel - 1, out-channels - 6, kernel size - 5, stride(default) - 1, padding(default) -0\n",
        "    self.pool = nn.MaxPool2d(2,2) # kernel size -2, stride - 2\n",
        "    self.conv2 = nn.Conv2d(6,16,5)\n",
        "    self.fc1 = nn.Linear(16*4*4,120)\n",
        "    self.fc2 = nn.Linear(120,84)\n",
        "    self.fc3 = nn.Linear(84,10)\n",
        "    # instantiate ReLU here\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    # apply ReLU using the instantiated object\n",
        "    x = self.pool(self.relu(self.conv1(x)))\n",
        "    x = self.pool(self.relu(self.conv2(x)))\n",
        "    x = x.view(-1,16*4*4)\n",
        "    x = self.relu(self.fc1(x))\n",
        "    x = self.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "83OisjsBIkfD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lenet = LeNet_MNIST().to(device)\n",
        "\n",
        "optimizer_lenet = torch.optim.SGD(model_lenet.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "\n",
        "model_lenet.train()\n",
        "for epoch in range(3):\n",
        "  for batch, (X,y) in enumerate(train_dataloader):\n",
        "    X,y = X.to(device), y.to(device) # porting from CPU to GPU\n",
        "    pred = model_lenet(X) # Forward Prapogation\n",
        "    loss = loss_fn(pred,y) # Loss Calculation\n",
        "\n",
        "    optimizer_lenet.zero_grad() # gradient reset to zero\n",
        "    loss.backward() # gradient computation\n",
        "    optimizer_lenet.step() # weight updation\n",
        "    if batch % 100 == 0:\n",
        "      cur_loss ,current = loss.item(), batch * len(X)\n",
        "      print(f\"loss: {cur_loss:>7f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHT9VMS_K1r0",
        "outputId": "77646ec0-b8db-4c23-d7c7-6fbfda2ba58d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.307795\n",
            "loss: 2.299750\n",
            "loss: 2.295894\n",
            "loss: 2.297123\n",
            "loss: 2.288498\n",
            "loss: 2.288259\n",
            "loss: 2.283167\n",
            "loss: 2.250010\n",
            "loss: 2.242318\n",
            "loss: 2.107315\n",
            "loss: 1.984722\n",
            "loss: 1.477033\n",
            "loss: 0.910455\n",
            "loss: 0.906018\n",
            "loss: 0.581109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the LeNet architecture\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)  # Input: 1x28x28, Output: 6x28x28\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)  # Input: 6x28x28, Output: 16x24x24\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # Fully connected layer\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)  # Output layer for 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.max_pool2d(x, kernel_size=2, stride=2)  # Output: 6x14x14\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.max_pool2d(x, kernel_size=2, stride=2)  # Output: 16x5x5\n",
        "        x = x.view(-1, 16 * 5 * 5)  # Flatten\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)  # Output logits\n",
        "        return x\n",
        "\n",
        "# Data preparation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to mean 0, std 1\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Model, loss function, optimizer\n",
        "model = LeNet()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Training loop\n",
        "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
        "\n",
        "# Testing loop\n",
        "def test(model, device, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "\n",
        "            # Sum up batch loss\n",
        "            test_loss += criterion(output, target).item()\n",
        "\n",
        "            # Get the index of the max log-probability\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n\")\n",
        "\n",
        "# Main training/testing process\n",
        "num_epochs = 10\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, criterion, epoch)\n",
        "    test(model, device, test_loader, criterion)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEPRpTjeLCBq",
        "outputId": "f39ba8e9-7c00-40a1-f1f0-8ea4b70f456a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.310493\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.804879\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.290695\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.145462\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.057088\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.186527\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.176995\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.095322\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.023060\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.150100\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9762/10000 (98%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.046983\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.009549\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.118597\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.123943\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.052845\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.068856\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.078094\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.048163\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.108177\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.011748\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9831/10000 (98%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.071804\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.012709\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.008402\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.018879\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.047040\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.034776\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.009901\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.045643\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.042639\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.056333\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9856/10000 (99%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.141687\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.071008\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.059318\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.005183\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.041131\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.033718\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.044769\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.015292\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.024803\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.077472\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9877/10000 (99%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.085610\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.001655\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.001252\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.018020\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.054200\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.003114\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.017879\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.080197\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.059499\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.002375\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9912/10000 (99%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.027357\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.012402\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.062837\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.015301\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.006733\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.008604\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.001595\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.016343\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.055447\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.001729\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9881/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.082433\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.007972\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.002385\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.010146\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.009758\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.001816\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.000485\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.006443\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.015168\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.038443\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9897/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.000188\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.004286\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.000651\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.001660\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.014203\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.099068\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.017982\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.102217\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.053877\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.001333\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9903/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.013760\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.025997\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.005472\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.000234\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.003029\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.015205\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.001384\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.000792\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.002307\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.003788\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9895/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.001692\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.006302\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.001899\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.001312\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.000152\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.012845\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.002779\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.008265\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.000127\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.001415\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9907/10000 (99%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iv1sM2bfNuXC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}